# Semana 1
## Kelleher, J. D., Tierney, B. (2018). Data Science. The MIT Press.
- However, although data science borrows from these other fields, it is broader in scope.
- Machine learning (ML) focuses on the design and evaluation of algorithms for extracting patterns from data.
- Data mining generally deals with the analysis of structured data and often implies an emphasis on com-
mercial applications. 
- Data science takes all of these consid- erations into account but also takes up other challenges,such as the capturing, cleaning, and transforming of unstructured social media and web data; the use of big- data technologies to store and process big, unstructured data sets; and questions related to data ethics and regulation.
- Using data science, we can extract different types of patterns.
- In business jargon, this task is known as customer segmentation, and in data science terminology it is called clustering. 
- Alternatively, we might want to extract a pattern that identifies products that are frequently bought together, a process called association- rule mining.
- Or we might want to extract patterns that identify strange or abnormal events, such as fraudulent insurance claims, a process known as anomaly or outlier detection.
- Finally, we might want to identify patterns that help us to classify things. For example, the following rule illustrates what a classification pattern extracted from an email data set might look like: If an email contains the phrase “Make money easily,” it is likely to be a spam email. Identifying these types of classification rules is known as prediction.
- data science becomes useful when we have a large number of data examples and when the patterns are too complex for humans to discover and extract manually
-  The phrase actionable insight is sometimes used in this context to describe what we want the extracted patterns to give us.
- The term insight highlights that the pattern should give us relevant information about the problem that isn’t ob- vious.
- The term actionable highlights that the insight we get should also be something that we have the capacity to use in some way.
- Transactional data include event information such as the sale of an item, the issuing of an invoice, the deliv- ery of goods, credit card payment, insurance claims, and so on.
- Nontransactional data, such as demographic data, also have a long history.
- Databases are the natural technology to use for storing and retrieving structured transactional or operational data (i.e., the type of data generated by a company’s day-to-day operations). However, as companies have become larger and more automated, the amount and variety of data generated by different parts of these companies have dra- matically increased
- This business chal- lenge led to the development of data warehouses. In a data warehouse, data are taken from across the organization and integrated, thereby providing a more comprehensive data set for analysis
- Big data are often defined in terms of the three Vs: the extreme volume of data, the va- riety of the data types, and the velocity at which the data must be processed.
- The existence of big data has also led to the develop- ment of new data-processing frameworks. When you are dealing with large volumes of data at high speeds, it can be useful from a computational and speed perspective to distribute the data across multiple servers, process que-
ries by calculating partial results of a query on each server, and then merge these results to generate the response to the query. This is the approach taken by the MapReduce framework on Hadoop. In the MapReduce framework, the data and queries are mapped onto (or distributed across) multiple servers, and the partial results calculated on each server are then reduced (merged) together.
- The method of least squares provided the foundation for statistical learning methods such as linear regression and logistic regression as well as the
development of artificial neural network models in artifi- cial intelligence
- William Playfair was inventing statistical graphics and laying the foundations for modern data visualization and exploratory data analysis
- Visualizations can also be useful to communicate the results of a data science project.
- In 1943, Warren McCulloch and Walter Pitts proposed the first mathematical model of a neural net- work. In 1948, Claude Shannon published “A Mathemati- cal Theory of Communication” and by doing so founded information theory. In 1951, Evelyn Fix and Joseph Hodges proposed a model for discriminatory analysis (what would now be called a classification or pattern-recognition prob- lem) that became the basis for modern nearest-neighbor models.
- The term data science came to prominence in the late 1990s in discussions relating to the need for statisticians to join with computer scientists to bring mathematical rigor to
the computational analysis of large data sets. 
- In most organizations, a significant portion of the data will come from the databases in the organization. Furthermore, as the data architecture of an organization grows, data science projects will start incorporating data from a variety of other data sources, which are commonly
referred to as “big-data sources.” The data in these data sources can exist in a variety of different formats, gener- ally a database of some form—relational, NoSQL, or Ha- doop. All of the data in these various databases and data sources will need to be integrated, cleansed, transformed, normalized, and so on. These tasks go by many names, such as extraction, transformation, and load, “data mung- ing,” “data wrangling,” “data fusion,” “data crunching,” and so on
- Finally, a key aspect of being a successful data scien- tist is being able to communicate the story in the data.

## Shekhar, P. (2022). Key Players/Jobs in the Data Ecosystem. Recuperado el 18 de enero del 2023 desde https://prashantshekhar.com/blog/key-players-jobs-in-the-data-ecosystem/
- it’s not uncommon for data professionals to start their career in one of the data roles and transition to another role within the data ecosystem by supplementing their skills
- You require a wide range of skill sets and individuals playing a variety of roles in order to extract value from data. 
- Data Engineers : Data Engineers are people who develop and maintain data architectures and make data available for business operations and analysis. Data Engineers work within the data ecosystem to extract, integrate, and organize data from disparate sources; clean, transform, and prepare data; design, store, and manage data in data repositories. They enable data to be accessible in formats and systems that the various business applications, as well as stakeholders like Data Analysts and Data Scientists, can utilize. A Data Engineer must have good knowledge of programming, sound knowledge of systems and technology architectures, and in-depth understanding of relational databases and non-relational datastores.

- Data Analyst : A Data Analyst translates data and numbers into plain language, so organizations can make decisions. Data Analysts inspect, and clean data for deriving insights; identify correlations, find patterns, and apply statistical methods to analyze and mine data; and visualize data to interpret and present the findings of data analysis. Analysts are the people who answer questions such as “Are the users’ search experiences generally good or bad with the search functionality on our site” or “What is the popular perception of people regarding our rebranding initiatives” or “Is there a correlation between sales of one product and another.” Data Analysts require good knowledge of spreadsheets, writing queries, and using statistical tools to create charts and dashboards. Modern data analysts also need to have some programming skills. They need strong analytical and story-telling skills

- Data Scientist : Data Scientists analyze data for actionable insights and build Machine Learning or Deep Learning models that train on past data to create predictive models. Data Scientists are people who answer questions such as “How many new social media followers am I likely to get next month?” or “What percentage of my customers am I likely to lose to competition in the next quarter” or “Is this financial transaction unusual for this customer?”. Data Scientists require knowledge of Mathematics, Statistics, and a fair understanding of programming languages, databases, and building data models. They also need to have domain knowledge.

- Business Analyst and Business Intelligence Analyst :  Business Analysts leverage the work of Data Analysts and Data Scientists to look at possible implications for their business and the actions they need to take or recommend. BI Analysts do the same, except their focus is on the market forces and external influences that shape their business. They provide business intelligence solutions by organizing and monitoring data on different business functions and exploring that data to extract insights and actionable that improve business performance.

## Kelleher, J. D., Tierney, B. (2018). Data Science. The MIT Press.
- CRISP-DM was originally developed by a consortium of organizations consisting of leading data science ven- dors, end users, consultancy companies, and researchers.
- The CRISP-DM life cycle consists of six stages: busi- ness understanding, data understanding, data preparation, modeling, evaluation, and deployment
- The process is semistructured, which means that a data scientist doesn’t always move through these six stages in a linear fashion.
- needs and the data that the business has available to it. In the early stages of a project, a data scientist will often iterate between focusing on the business and exploring what data are available. This iteration typically involves identifying a business problem and then exploring if the appropriate data are available to develop a data-driven so- lution to the problem. If the data are available, the project can proceed; if not, the data scientist will have to identify an alternative problem to tackle.
- Once the data scientist has clearly defined a busi- ness problem and is happy that the appropriate data are available, she moves on to the next phase of the CRISP- DM: data preparation
- . The focus of the data-preparation stage is the creation of a data set that can be used for the data analysis. In general, creating this data set involves integrating data sources from a number of databases.
- Checking the quality of the data is very important because errors in the data can have a serious effect on the performance of the data-analysis algorithms.
- The next stage of CRISP-DM is the modeling stage. This is the stage where automatic algorithms are used to extract useful patterns from the data and to create models that encode these patterns. Machine learning is the field of computer science that focuses on the design of these al- gorithms. In the modeling stage, a data scientist will nor- mally use a number of different ML algorithms to train
a number of different models on the data set.
- At this stage in the project, the data scientist typically doesn’t know which patterns are the best ones to look for in the data, so in this context it makes sense to experiment with a number of different algorithms
- In the majority of data science projects, the initial model test results will uncover problems in the data.
- The last two stages of the CRISP-DM process, evaluation and deployment, 
- the tests run during the modeling stage are focused purely on the accuracy of the models for the data set. The evaluation phase involves assessing the models in the broader context defined by the business needs. 
- it is also useful for the data scientist to do a general quality- assurance review on the project activities: Was anything
missed? Could anything have been done better
- the main decision made during the evaluation phase is whether any of the models should be deployed in the business or another it- eration of the CRISP-DM process is required to create ad- equate models. 
- Assuming the evaluation process approves a model or models, the project moves into the final stage of the process: deployment. The deployment phase in- volves examining how to deploy the selected models into the business environment. This involves planning how to integrate the models into the organization’s techni- cal infrastructure and business processes. The best mod-
els are the ones that fit smoothly into current practices.
- Models that fit current practices have a natural set of users who have a clearly defined problem that the model helps them to solve. Another aspect of deployment is putting a plan in place to periodically review the performance of the model.

## Vaughan, D. (2020). Analytical Skills for Al and Data Science. O'Reilly Media. Recuperado el 18 de enero del 2023 desde https://learning.oreilly.com/library/view/analytical-skills-for/9781492060932/ch02.html#idm46388894678408
- In a nutshell, descriptive relates to how things are, predictive to how we believe things will be, and prescriptive to how things ought to be.
-  since description relates to the current state and prescription to the quality of decisions, and prediction is an input to make decisions, which may or may not be optimal or even good
- This is a great example of what can be achieved with descriptive analysis, thanks to our remarkable ability to recognize patterns in the data. Here we quickly identified a change in the trend (churn is accelerating), the existence of strong seasonal effects, and a positive correlation between churn rates and average household income in the scatterplot.
- We will have the opportunity to go into greater detail on this use case, but let me just single out two characteristics of any prescriptive analysis: as opposed to the two previous analyses, here we actively recommend courses of action that can improve our position, by way of incentivizing a likely-to-leave customer to stay longer with us. Second, prediction is used as an input in the decision-making process, helping us calculate expected savings and costs. 

- Defining the business question
- we start with an action and not with the business objective, so we can apply the sequence of why questions, and most likely we’ll end up with the simple fact that customers are our main source of recurring revenues
- but this simple fact takes us to the main KPI we want to maximize:

- descriptive
- We may start at the most aggregate level by looking at time trends and patterns of seasonality, which gives us a sense of our current health status. But data has the power to go deeper and tell us who the customers are that have already left
- We can get as granular as our data and time allow. But you get the idea: this is just a snapshot, and hopefully I have convinced you by now that no matter how high-definition it is, it’s hard to get more value out of it.

- predictive
- AI and machine learning can help us find answers to the predictive question: can we anticipate which customers are more likely to leave? Thanks to the richness of our descriptive analysis, we have hopefully now found some of the primary drivers that explain our current churn rate.
- The best data scientists are those who understand and hypothesize why customers are leaving. In this way they can create more specific predictors in a process called feature engineering, which is the best way to get really good predictive power. Knowing what to include or not in our models is the holy grail in the construction of good models, even more than, say, choosing the ever-more powerful available algorithms.


- prescriptive
- Finally, we have arrived at the prescriptive question: what levers should we pull if we want to maximize our profits from our retention campaigns? But are we thinking of short-term profits?
- 
