# Aprendizaje supervisado
-  el aprendizaje supervisado se utiliza un conjunto de datos etiquetados (denominado conjunto de entrenamiento) para entrenar un algoritmo. El conjunto de datos de entrenamiento incluye datos de entrada (características) asociados con valores de respuesta o salida.
- Con el modelo “aprendido” es posible asignar la etiqueta de salida a un nuevo valor de un conjunto de datos de prueba, o datos no vistos, para predecir el rendimiento y validar el algoritmo. Para crear el conjunto de prueba deberás elegir algunas instancias al azar, generalmente el 20% del conjunto de datos (o menos si tu conjunto es muy grande). scikit-learn proporciona algunas funciones para dividir conjuntos de datos en múltiples subconjuntos, pero la más simple es train_test_split()
- Regresión: Modelo de aprendizaje automático que genera una respuesta numérica continua. Por ejemplo, predecir los precios de las acciones, las condiciones meteorológicas, o el tiempo de desplazamiento entre localidades de la ciudad.
- Clasificación: Modelo de aprendizaje automático que genera una predicción a partir de un conjunto discreto de posibles resultados; es decir, la variable objetivo es de tipo categórico. Por ejemplo, predecir si una imagen médica indica un estado saludable o cáncer es un problema de clasificación binaria porque tiene dos clases, mientras que un modelo que se utiliza para segregar los correos electrónicos en varias categorías (social, educación, trabajo y familia) es un problema de clasificación multiclase.
- Para tener una idea del rendimiento de un modelo en todo el conjunto de datos de entrenamiento, se pueden promediar todos los residuos. Sin embargo, los residuos positivos y negativos se cancelarían y darían como resultado un promedio pequeño, incluso si tu modelo es deficiente. Entonces, para abordar este problema, los residuos se elevan al cuadrado antes del promedio, lo que garantiza que todos los valores sean positivos. Este proceso de elevar al cuadrado cada residuo individual y luego tomar el promedio de los errores cuadrados da como resultado el error cuadrático medio (MSE - Mean Squared Error) de tu modelo. Cuando se entrena el modelo, se encuentran los coeficientes que minimizan este valor.
- La inserción de los cuadrados hace que el MSE no tenga las misma unidades que la variable de respuesta. Por esta razón, el valor reportado con más frecuencia es la raíz cuadrada del error cuadrático medio o RMSE (Root Mean Squared Error)
- Para el algoritmo, esta ecuación es esencialmente lo mismo que considerar dos variables predictoras únicas. Además, al igual que se pueden generar términos polinomiales multiplicando una variable consigo misma, también se pueden crear nuevos términos multiplicando diferentes variables juntas. Estas variables predictoras adicionales se denominan términos de interacción y estos son útiles cuando la variable de respuesta depende del producto combinado de predictores, además de sus valores individuales.

## IBM (2022). What is supervised learning? https://www.ibm.com/topics/supervised-learning
- is a subcategory of machine learning and artificial intelligence. It is defined by its use of labeled datasets to train algorithms that to classify data or predict outcomes accurately.
- As input data is fed into the model, it adjusts its weights until the model has been fitted appropriately, which occurs as part of the cross validation process. Supervised learning helps organizations solve for a variety of real-world problems at scale, such as classifying spam in a separate folder from your inbox.

- The algorithm measures its accuracy through the loss function, adjusting until the error has been sufficiently minimized.
- Classification uses an algorithm to accurately assign test data into specific categories. It recognizes specific entities within the dataset and attempts to draw some conclusions on how those entities should be labeled or defined. Common classification algorithms are linear classifiers, support vector machines (SVM), decision trees, k-nearest neighbor, and random forest, which are described in more detail below.
- Regression is used to understand the relationship between dependent and independent variables. It is commonly used to make projections, such as for sales revenue for a given business. Linear regression, logistical regression, and polynomial regression are popular regression algorithms.

- Neural networks: Primarily leveraged for deep learning algorithms, neural networks process training data by mimicking the interconnectivity of the human brain through layers of nodes. Each node is made up of inputs, weights, a bias (or threshold), and an output. If that output value exceeds a given threshold, it “fires” or activates the node, passing data to the next layer in the network. Neural networks learn this mapping function through supervised learning, adjusting based on the loss function through the process of gradient descent. When the cost function is at or near zero, we can be confident in the model’s accuracy to yield the correct answer.
- Naive bayes: Naive Bayes is classification approach that adopts the principle of class conditional independence from the Bayes Theorem. This means that the presence of one feature does not impact the presence of another in the probability of a given outcome, and each predictor has an equal effect on that result. There are three types of Naïve Bayes classifiers: Multinomial Naïve Bayes, Bernoulli Naïve Bayes, and Gaussian Naïve Bayes. This technique is primarily used in text classification, spam identification, and recommendation systems.
- Linear regression: Linear regression is used to identify the relationship between a dependent variable and one or more independent variables and is typically leveraged to make predictions about future outcomes. When there is only one independent variable and one dependent variable, it is known as simple linear regression. As the number of independent variables increases, it is referred to as multiple linear regression. For each type of linear regression, it seeks to plot a line of best fit, which is calculated through the method of least squares. However, unlike other regression models, this line is straight when plotted on a graph.
- Logistic regression: While linear regression is leveraged when dependent variables are continuous, logistic regression is selected when the dependent variable is categorical, meaning they have binary outputs, such as "true" and "false" or "yes" and "no." While both regression models seek to understand relationships between data inputs, logistic regression is mainly used to solve binary classification problems, such as spam identification.
- Support vector machines (SVM): A support vector machine is a popular supervised learning model developed by Vladimir Vapnik, used for both data classification and regression. That said, it is typically leveraged for classification problems, constructing a hyperplane where the distance between two classes of data points is at its maximum. This hyperplane is known as the decision boundary, separating the classes of data points (e.g., oranges vs. apples) on either side of the plane.
- K-nearest neighbor: K-nearest neighbor, also known as the KNN algorithm, is a non-parametric algorithm that classifies data points based on their proximity and association to other available data. This algorithm assumes that similar data points can be found near each other. As a result, it seeks to calculate the distance between data points, usually through Euclidean distance, and then it assigns a category based on the most frequent category or average. Its ease of use and low calculation time make it a preferred algorithm by data scientists, but as the test dataset grows, the processing time lengthens, making it less appealing for classification tasks. KNN is typically used for recommendation engines and image recognition.
- Random forest: Random forest is another flexible supervised machine learning algorithm used for both classification and regression purposes. The "forest" references a collection of uncorrelated decision trees, which are then merged together to reduce variance and create more accurate data predictions.

### Supervised learning examples
- Image- and object-recognition: Supervised learning algorithms can be used to locate, isolate, and categorize objects out of videos or images, making them useful when applied to various computer vision techniques and imagery analysis.
- Predictive analytics: A widespread use case for supervised learning models is in creating predictive analytics systems to provide deep insights into various business data points. This allows enterprises to anticipate certain results based on a given output variable, helping business leaders justify decisions or pivot for the benefit of the organization.
- Customer sentiment analysis: Using supervised machine learning algorithms, organizations can extract and classify important pieces of information from large volumes of data—including context, emotion, and intent—with very little human intervention. This can be incredibly useful when gaining a better understanding of customer interactions and can be used to improve brand engagement efforts.
- Spam detection: Spam detection is another example of a supervised learning model. Using supervised classification algorithms, organizations can train databases to recognize patterns or anomalies in new data to organize spam and non-spam-related correspondences effectively.

### Challenges
- Supervised learning models can require certain levels of expertise to structure accurately.
- Training supervised learning models can be very time intensive.
- Datasets can have a higher likelihood of human error, resulting in algorithms learning incorrectly.
- Unlike unsupervised learning models, supervised learning cannot cluster or classify data on its own.

## Géron, A. (2022). Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow (3rd ed.). O´Reilly Media.
- It’s time to prepare the data for your machine learning algorithms. Instead of doing this manually, you should write functions for this purpose, for several good reasons:

### clean
- Most machine learning algorithms cannot work with missing features, so you’ll need to take care of these. For example, you noticed earlier that the total_bedrooms attribute has some missing values. You have three options to fix this:
- You can accomplish these easily using the Pandas DataFrame’s dropna(), drop(), and fillna() methods:
- ou decide to go for option 3 since it is the least destructive, but instead of the preceding code, you will use a handy Scikit-Learn class: SimpleImputer. 
- The imputer has simply computed the median of each attribute and stored the result in its statistics_ instance variable. Only the total_bedrooms attribute had missing values, but you cannot be sure that there won’t be any missing values in new data after the system goes live, so it is safer to apply the imputer to all the numerical attributes:


## Ujhelyi, T. (2021, noviembre 16). Polynomial Regression in Python using scikit-learn 